\documentclass{article}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{footnote}
\makesavenoteenv{tabular}

% Title content
\title{Transferability of Adversarial Reinforcement Learning}
\author{Connor Fuhrman, Johnathan Gill}
\date{May 11 2021}

\begin{document}

\maketitle

\begin{abstract}
It is well known that deep learning and deep reinforcement learning (RL) are vulnerable to adversarial attacks via perturbations applied to a model's input \cite{Szegedy2014}. 
Recently, it was shown that RL is susceptible to adversarial attacks on the \textit{policy} alone via an attacker trained to develop an adversarial policy \cite{Gleave2019}. 
Such attacks are especially interesting because the adversarial attacker does not simply perturb the victim's model's input, i.e., the observation space in the domain of RL, but rather choses physically realistic actions such that the victim's policy breaks down. 
This method of adversarial attack is critical to understand more deeply because real-world RL agents may only interact with each other and the environment around them in a physically realistic fashion and may not artificially perturb the observation space of another agent. 
This work investigates the transferability of such adversarial policies between various model architectures via the \textit{Shoot and Defend} zero-sum game. 
Quadcopter agents are trained to be either the shooter, who's goal is to shoot a ball into a goal, or a defender, who's goal is to keep the ball out of the goal. 
There are two shooter agents trained with distinctly different model architectures and two keeper agents, one of which is trained adversarially against one shooter agent but \textbf{not} the other. 
We find that ...
\end{abstract}

\newpage

\section{Introduction}\label{sec:intro}
Adversarial attacks against neural network classifiers \cite{Szegedy2014} and (deep) reinforcement learning (RL) \cite{Gleave2019} has been shown effective via adversarial perturbations of the victim model's input, e.g., altering pixel values for an image classifier or perturbing an observation space for an RL model. 
However, such attacks are artificial when considering real-world RL agents as an adversarial attacker cannot directly alter, e.g., image pixel or observation space values, but can only interact naturally with the environment and the victim model, e.g., an adversarial attack against an autonomous unmanned aerial vehicle (UAV) cannot access the UAV's perception equipment directly to alter measurement values nor can it alter the environment in a non-physically realistic manner but can only conduct physically-realistic actions, e.g., a flight pattern, which serves to attack and exploit an agent's policy.  \\

\noindent 
Work done in \cite{Gleave2019} showed that an adversarial policy was effective in multiple zero-sum games, including \textit{Shoot and Defend} where the \texttt{shooter} agent attempts to shoot a ball into a goal while the \texttt{keeper} agent seeks to keep the ball out of the goal. 
The adversarial attacker, the keeper agent, demonstrated significantly different activations than the normal models and was overall successful in each game by manipulating only it's own body position to exploit weaknesses in the victim's (the shooter agent) policy.
Videos showing the results from \cite{Gleave2019} are available \href{https://adversarialpolicies.github.io/}{online} and show that adversarial policies are more effective in higher-dimensional spaces such as humanoid models over lower-dimensional spaces such as ants. 

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{imgs/Gleave2019_env}
  \caption{Training environment used in \cite{Gleave2019} showing all four zero-sum games investigated for adversarial policies}
  \label{fig:gleave_env}
\end{figure}

\noindent 
The adversarial agents in \cite{Gleave2019} were given unlimited black-box access to the victim's policy, $\pi_v$, but were not given any white-box access to model weights or activations. 
The victim model's policy was frozen and actions were sampled from the stochastic policy such that $a_v$ is a sample of the stochastic policy $\pi_v(. | s)$ while the adversary developed a policy $\pi_a$ such that $\pi_a$ maximizes the discounted sum of rewards. 
Agents trained adversarially demonstrate reliable domination of the vicim models, winning upwards of 90\% of games in most cases. 
However, it is interesting to note that when the victim is "blind" to the attacker, i.e., the victim's observations corresponding to the attacker's position is set statically to a typical initial value, its performance increases significantly from losing over 80\% of games to winning 99\% of games in \textit{You Shall Not Pass} (this game demonstrates the phenomenon best but the trend exists in other games \cite{Gleave2019} ). 

\section{Methods}\label{sec:methods}
The following sections discuss the gym environment, RL framework, and deployment on the high performance computing (HPC) cluster at the University of Arizona.

\subsection{Gym Environment}\label{subsec:gym_env}
We utilize an OpenAI-based gym environment introduced in \cite{Panerati2021} which provides a multi-agent RL environment based on the \texttt{Bullet} physics engine. 
The environment, \texttt{gym-pybullet-drones}, (original repository available online at \url{github.com/utiasDSL/gym-pybullet-drones}) allows users to rapidly create new environments and instantiate models within defined by their Unified Robot Description Format (URDF). \\

\noindent
The drone model utilized in this work is the \href{https://www.bitcraze.io/documentation/hardware/crazyflie_2_1/crazyflie_2_1-datasheet.pdf}{Bitcraze Cracyflie 2.1} as this model ships with the gym environment. 
The quadcopter itself is a small open-source development platform which weight under 30g and is intended for education, research, and in particular swarm applications. 
Note, however, that this drone model in particular is not integral to this work but was chosen simply because it was readily available and already integrated/tested within the gym environment.
The use of quadcopters was chosen because \cite{Gleave2019}  showed that agents were more susceptible to adversarial attacked in higher dimensional spaces and quadcopters, as they are aerial vehicles, introduce a vertical component. 

\subsection{Shoot and Defend Environment}\label{subsec:sad_env}
The \textit{Shoot and Defend} environment is configured as in Figure~\ref{fig:sad_env}. 
\begin{figure}[t!]
  \includegraphics[width=\linewidth]{imgs/ShootAndDefend_env}
  \caption{Diagram depicting the \textit{Shoot and Defend} environment. Note this is a top-down view as the shooter and defender may travel in the vertical direction.}
  \label{fig:sad_env}
\end{figure}
The shooter and defender agents are each constrained to remain in their respective areas and receive a punishment if this boundary is violated. 
The shooter agent receives a reward if the ball enters the goal area and a penalty if the ball goes out of bounds or becomes stationary. 
Both agents also receive penalties for crashing and other undesirable behavior. 
The exact rewards and conditions are shown in Appendix~\ref{app:rewards}. \\

\noindent
The observation space for each agent is the physical properties of all physical objects in the scene, i.e., the position, attitude, velocity, and angular velocity. 
However, in order to reduce the complexity and without loss of generality the observations for the ball object are simply position and velocity as the ball's attitude and angular velocity \footnote{The ball is shot with a force of 50N and there is no external force applied by moving air. The ball is then essentially only affected by gravity and would not experience a curved trajectory as in a windy environment} do not play as significant a role in the game as the position and velocity. 
Therefore, the dimensionality for the observation space of each agent is a 30-length vector for both the shooter and defender agents. 
The action space for the defender agent is an 8-vector representing the mean and standard deviation of a Gaussian distribution by which motor revolution per minute (RPM) values are sampled as this encourages early exploration as the standard deviation decreases over training iterations. 
The shooter agent adds an additional mean and standard deviation pair which indicates that the shooter agent should shoot the ball. 
If the value of this sampled distribution is above a threshold, the shooter launches the ball in the drone platform's current orientation \footnote{Note that the ball is instantiated as a model at the time of shooting and the shooter agent does not carry the ball beforehand.}. 

\subsection{Training Methodology}\label{subsec:train_methods}
Both the shooter and defender agents are trained using Proximal Policy Optimization (PPO) \cite{Schulman2017} via the Ray RLlib library \cite{Liang2017} which provides RL-specific libraries and functionality atop the Ray API for distributed operation.
A policy gradient method, in particular PPO, was chosen over Q networks due to the method's ability to handle continuous action spaces, i.e., the quadcopter's motor RPM commands. 
Policy gradient methods have been demonstrated to achieve state-of-the-art performance using deep neural networks in multiple application areas including, but not limited to, Go, Atari and other video games, and 3D locomotion \cite{ppo_site}. 
PPO addresses issues with various other policy optimization methods such as sensitivity to step size and poor sample efficiency while avoiding overcomplicated implementations, e.g., Actor-Critic with Experience Replay (ACER) \cite{wang2017sample} which introduces more implementation complexity via off-policy correction and a replay buffer while showing only marginal improvement over PPO in OpenAI's Atari benchmark \cite{ppo_site}, or algorithms, e.g, Trust Region Policy Optimization (TRPO) \cite{schulman2017trust}, which are unable to share parameters between the policy and value function or auxiliary losses such as dropout (e.g., Atari games or other domains heavily-reliant on visual input \cite{ppo_site}). 
PPO utilizes the objective function
\begin{equation} \label{eq:ppo_loss}
L^{CLIP}(\theta) = \hat{E}_t[min(r_t(\theta) \hat{A}_t , clip(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t]
\end{equation}
where
\begin{itemize}
\item $\theta$ is the policy parameter
\item $\hat{E}_t$ is the empirical expectation over timesteps
\item $r_t$ is the ratio of probabilities under the new and old policies respectively
\item $\hat{A}_t$ is the estimated advantage at time $t$ 
\item $\epsilon$ is a hyperparameter \footnote{The authors of \cite{Schulman2017} and \cite{ppo_site} recommend usually 0.2 or 0.3}
\end{itemize}

\noindent
We utilize the PPO implementation via Ray RLlib \cite{ray_ppo} as the Ray library provides abstractions for parallelization and distributed workflows and the RLlib library contains a suite of highly parallelizable GPU-enabled algorithms which are (mostly) compatible with both TensorFlow and PyTorch. 
Figure~\ref{fig:ray_blockdiagram} shows the relationship between Ray, the gym environment, and RLlib algorithms. 

\begin{figure}
  \includegraphics[width=\linewidth]{imgs/ray_blockdiagram}
  \caption{Ray library conceptual block diagram \cite{ray_ppo}. The \textit{Shoot and Defend} environment created herein is an OpenAI-compatible Gym and we utilize RLlib algorithms which utilize Ray's abstraction layer to effectuate distributed execution.}
  \label{fig:ray_blockdiagram}
\end{figure}

\noindent 
The complete Ray configuration is shown in Appendix~\ref{app:ray_config}.  

TODO: Discuss model configuration such as LSTM and what the different models were.

\subsection{Containerization via Singularity and Deployment on High Performance Computing (HPC) Cluster}\label{container_hpc}
In order to effectively train multiple RL agents with varying model architectures, a GPU-enabled  high-performance computing (HPC) cluster is all-but necessary. 
The University of Arizona (UArizona) provides three distinct HPC clusters with varying CPU/GPU/RAM capabilities. 
We utilize the El Gato supercomputer cluster which boasts 131 compute nodes each with 2 Xeon E5-2650v2 8-core (Ivy Bridge) CPUs for a total of 16 cores/node running at 2.66GHz. 
90 of El Gatoâ€™s nodes are equipped with an NVIDIA K20X (47 of the 90 nodes are equipped with 2x GPUs) and have 256GB memory. 
The remaining 41 CPU-only nodes are equipped with 64GB memory. \\ 

\noindent
In order to reliably deploy our learning framework across platforms and operating systems, the entire gym was containerized using Singularity \cite{singularity}. 
Containers are independent, cross-platform software packages which incorporate any dependancies into the container image and effectuate reliably reproducible, i.e., exactly the same, operation between host platforms, i.e., hardware or software. 
The \texttt{gym-pybullet-drones} gym is designed and tested on Ubuntu 18.04 and therefore a container was created based on the Ubuntu 18.04 image which, despite El Gato running CentOS 6 \footnote{El Gato, despite being the oldest and least capable of the UArizona's three HPC clusters, was chosen because the Ocelote and Puma HPC clusters both run CentOS 7 which has a kernel too new for compatibility with Ubuntu 18.04}, allows for training to take place on an Ubuntu operating system \footnote{Note that instillation of the required Python packages for the gym environment was unsuccessful within a virtual environment on the native CentOS 6 or 7 host and therefore containerization is absolutely required for our application}. 
Singularity, however, is a Linux-only program \footnote{At the time of this writing there are beta releases for MacOS} and many scientists and researchers run MacOS or Windows so Singularity was designed to "bootstrap" from a preexisting Docker \cite{docker} container as Docker supports Linux container execution on MacOS, Windows, and Linux but requires a daemon for execution (which does not easily integrate into a shared resource system such as an HPC) and does not easily utilize GPU resources. 
The \textit{Shoot and Defend} environment within the \texttt{gym-pybullet-drones} gym was therefore containerized first into a Docker container \footnote{This was chosen simply to test execution on a MacOS host system before deployment on the HPC} \footnote{The Docker container is available via \href{https://hub.docker.com/repository/docker/connorfuhrman/gym-pybullet-drones}{connorfuhrman/gym-pybullet-drones}} then bootstrapped into a Singularity container which was used for all training. 

\section{Results} \label{sec:results}
TODO: this whole section ... 

\section{Conclusion} \label{sec:conclusion}
TODO: this whole section ... 
\subsection{Future Work} \label{subsec:future_work}
TODO: maybe keep this if we get any interesting results ... 

\clearpage 
\appendix
\section{Agent Rewards}\label{app:rewards}
\subsection{Defender Agent Rewards}
\begin{tabular}{ | c | l | }
\hline
Value & Condition \\ \hline
0 & The shooter moves outside it's designated box \\ \hline
0 & The shooter crashes \\ \hline
-500 & The shooter scores a goal \\ \hline 
-10 & The defender crashes \\ \hline
-10 & The defender moves outside it's designated box \\ \hline 
0 & The ball goes out of bounds \\ \hline 
5 & The ball is stationary \footnote{I.e., the ball was shot, did not enter the goal, and the ball's velocity is below a small threshold, e.g., $10^{-6}$} \\ \hline
-8 & The alloted time per episode is exceeded \\ \hline 
$10^{-2}$ & Reward to encourage level flight \\ \hline
\end{tabular}

\subsection{Shooter Agent Rewards}
\begin{tabular}{ | c | l | }
\hline
Value & Condition \\ \hline
0 & The defender moves outside it's designated box \\ \hline
0 & The defender crashes \\ \hline
500 & The shooter scores a goal \\ \hline 
-10 & The shooter crashes \\ \hline
-10 & The shooter moves outside it's designated box \\ \hline 
0 & The ball goes out of bounds \\ \hline 
-5 & The ball is stationary  \\ \hline
-8 & The alloted time per episode is exceeded \\ \hline 
$10^{-2}$ & Reward to encourage level flight \\ \hline
$10^{-2}$ & Reward to encourage the ball to move towards the goal \footnote{This reward encourages the shooter to shoot rather than not} \\ \hline
\end{tabular}

\section{Ray PPO Configuration} \label{app:ray_config}
TODO


\newpage
\bibliographystyle{plain}
\bibliography{reference.bib}

\end{document}