% Adversarial policies: Attacking deep reinforcement learning =====================
@article{Gleave2019,
abstract = {Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent.},
archivePrefix = {arXiv},
arxivId = {1905.10615},
author = {Gleave, Adam and Dennis, Michael and Kant, Neel and Wild, Cody and Levine, Sergey and Russell, Stuart},
eprint = {1905.10615},
file = {:Users/connorfuhrman/Downloads/1905.10615-2.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--16},
title = {{Adversarial policies: Attacking deep reinforcement learning}},
year = {2019}
}% ===================================================================


% gym-pybullet-drones paper ==============================================
@article{Panerati2021,
abstract = {Robotic simulators are crucial for academic research and education as well as the development of safety-critical applications. Reinforcement learning environments -- simple simulations coupled with a problem specification in the form of a reward function -- are also important to standardize the development (and benchmarking) of learning algorithms. Yet, full-scale simulators typically lack portability and parallelizability. Vice versa, many reinforcement learning environments trade-off realism for high sample throughputs in toy-like problems. While public data sets have greatly benefited deep learning and computer vision, we still lack the software tools to simultaneously develop -- and fairly compare -- control theory and reinforcement learning approaches. In this paper, we propose an open-source OpenAI Gym-like environment for multiple quadcopters based on the Bullet physics engine. Its multi-agent and vision based reinforcement learning interfaces, as well as the support of realistic collisions and aerodynamic effects, make it, to the best of our knowledge, a first of its kind. We demonstrate its use through several examples, either for control (trajectory tracking with PID control, multi-robot flight with downwash, etc.) or reinforcement learning (single and multi-agent stabilization tasks), hoping to inspire future research that combines control theory and machine learning.},
archivePrefix = {arXiv},
arxivId = {2103.02142},
author = {Panerati, Jacopo and Zheng, Hehui and Zhou, SiQi and Xu, James and Prorok, Amanda and Schoellig, Angela P.},
eprint = {2103.02142},
file = {:Users/connorfuhrman/Downloads/2103.02142.pdf:pdf},
title = {{Learning to Fly -- a Gym Environment with PyBullet Physics for Reinforcement Learning of Multi-agent Quadcopter Control}},
url = {http://arxiv.org/abs/2103.02142},
year = {2021}
}
% ===================================================================

% Intriguing properties paper ==============================================
@article{Szegedy2014,
abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
archivePrefix = {arXiv},
arxivId = {1312.6199},
author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
eprint = {1312.6199},
file = {:Users/connorfuhrman/Downloads/1312.6199.pdf:pdf},
journal = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
pages = {1--10},
title = {{Intriguing properties of neural networks}},
year = {2014}
}
% ===================================================================

% PPO paper ==========================================================
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
file = {:Users/connorfuhrman/Library/Application Support/Mendeley Desktop/Downloaded/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:pdf},
pages = {1--12},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
% ===================================================================

% Ray RL Lib paper =====================================================
@article{Liang2017,
abstract = {Reinforcement learning (RL) algorithms involve the deep nesting of highly irregular computation patterns, each of which typically exhibits opportunities for distributed computation. We argue for distributing RL components in a composable way by adapting algorithms for top-down hierarchical control, thereby encapsulating parallelism and resource requirements within short-running compute tasks. We demonstrate the benefits of this principle through RLlib: a library that provides scalable software primitives for RL. These primitives enable a broad range of algorithms to be implemented with high performance, scalability, and substantial code reuse. RLlib is available as part of the open source Ray project 1.},
author = {Liang, Eric and Liaw, Richard and Moritz, Philipp and Nishihara, Robert and Fox, Roy and Goldberg, Ken and Gonzalez, Joseph E. and Jordan, Michael I. and Stoica, Ion},
file = {:Users/connorfuhrman/Downloads/liang18b.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Rllib: Abstractions for distributed reinforcement learning}},
year = {2017}
}
======================================================================

@online{ppo_site,
  author = {John Schulman, Oleg Klimov, Filip Wolski, Prafulla Dhariwal, Alec Radford},
  title = {Proximal Policy Optimization},
  year = 2017,
  url = {https://openai.com/blog/openai-baselines-ppo/#ppo},
  urldate = {2021-05-07}
}

@online{ray_ppo, 
  author = {The Ray Team},
  tutle = {Ray Documentation: PPO}, 
  year = 2021, 
  url={https://docs.ray.io/en/master/rllib-algorithms.html#ppo},
  urldate = {2021-05-07}
}

@misc{wang2017sample,
      title={Sample Efficient Actor-Critic with Experience Replay}, 
      author={Ziyu Wang and Victor Bapst and Nicolas Heess and Volodymyr Mnih and Remi Munos and Koray Kavukcuoglu and Nando de Freitas},
      year={2017},
      eprint={1611.01224},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{schulman2017trust,
      title={Trust Region Policy Optimization}, 
      author={John Schulman and Sergey Levine and Philipp Moritz and Michael I. Jordan and Pieter Abbeel},
      year={2017},
      eprint={1502.05477},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{singularity,
    doi = {10.1371/journal.pone.0177459},
    author = {Kurtzer, Gregory M. AND Sochat, Vanessa AND Bauer, Michael W.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Singularity: Scientific containers for mobility of compute},
    year = {2017},
    month = {05},
    volume = {12},
    url = {https://doi.org/10.1371/journal.pone.0177459},
    pages = {1-20},
    abstract = {Here we present Singularity, software developed to bring containers and reproducibility to scientific computing. Using Singularity containers, developers can work in reproducible environments of their choosing and design, and these complete environments can easily be copied and executed on other platforms. Singularity is an open source initiative that harnesses the expertise of system and software engineers and researchers alike, and integrates seamlessly into common workflows for both of these groups. As its primary use case, Singularity brings mobility of computing to both users and HPC centers, providing a secure means to capture and distribute software and compute environments. This ability to create and deploy reproducible environments across these centers, a previously unmet need, makes Singularity a game changing development for computational science.}

}

@article{docker, author = {Merkel, Dirk}, title = {Docker: Lightweight Linux Containers for Consistent Development and Deployment}, year = {2014}, issue_date = {March 2014}, publisher = {Belltown Media}, address = {Houston, TX}, volume = {2014}, number = {239}, issn = {1075-3583}, abstract = {Docker promises the ability to package applications and their dependencies into lightweight containers that move easily between different distros, start up quickly and are isolated from each other.}, journal = {Linux J.}, month = mar, articleno = {2} }