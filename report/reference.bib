% Adversarial policies: Attacking deep reinforcement learning
@article{Gleave2019,
abstract = {Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent.},
archivePrefix = {arXiv},
arxivId = {1905.10615},
author = {Gleave, Adam and Dennis, Michael and Kant, Neel and Wild, Cody and Levine, Sergey and Russell, Stuart},
eprint = {1905.10615},
file = {:Users/connorfuhrman/Downloads/1905.10615-2.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--16},
title = {{Adversarial policies: Attacking deep reinforcement learning}},
year = {2019}
}

@article{Panerati2021,
abstract = {Robotic simulators are crucial for academic research and education as well as the development of safety-critical applications. Reinforcement learning environments -- simple simulations coupled with a problem specification in the form of a reward function -- are also important to standardize the development (and benchmarking) of learning algorithms. Yet, full-scale simulators typically lack portability and parallelizability. Vice versa, many reinforcement learning environments trade-off realism for high sample throughputs in toy-like problems. While public data sets have greatly benefited deep learning and computer vision, we still lack the software tools to simultaneously develop -- and fairly compare -- control theory and reinforcement learning approaches. In this paper, we propose an open-source OpenAI Gym-like environment for multiple quadcopters based on the Bullet physics engine. Its multi-agent and vision based reinforcement learning interfaces, as well as the support of realistic collisions and aerodynamic effects, make it, to the best of our knowledge, a first of its kind. We demonstrate its use through several examples, either for control (trajectory tracking with PID control, multi-robot flight with downwash, etc.) or reinforcement learning (single and multi-agent stabilization tasks), hoping to inspire future research that combines control theory and machine learning.},
archivePrefix = {arXiv},
arxivId = {2103.02142},
author = {Panerati, Jacopo and Zheng, Hehui and Zhou, SiQi and Xu, James and Prorok, Amanda and Schoellig, Angela P.},
eprint = {2103.02142},
file = {:Users/connorfuhrman/Downloads/2103.02142.pdf:pdf},
title = {{Learning to Fly -- a Gym Environment with PyBullet Physics for Reinforcement Learning of Multi-agent Quadcopter Control}},
url = {http://arxiv.org/abs/2103.02142},
year = {2021}
}
